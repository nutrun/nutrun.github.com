<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Parallelize by process :: nutrun</title>
    <link href="http://nutrun.com/feed.atom" rel="alternate" title="nutrun Atom feed" type="application/atom+xml" />
    <link href="http://nutrun.com/feed.rss" rel="alternate" title="nutrun RSS feed" type="application/rss+xml" />
    <link charset="utf-8" href="/css/reset.css" media="all" rel="stylesheet" type="text/css" />
    <link charset="utf-8" href="/css/screen.css" media="all" rel="stylesheet" type="text/css" />
    <link charset="utf-8" href="http://fonts.googleapis.com/css?family=IM+Fell+English" media="all" rel="stylesheet" type="text/css" />
    <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.4.2/jquery.min.js" type="text/javascript"></script>
    <script src="/js/app.js" type="text/javascript"></script>
  </head>
  <body>
    <div id="site">
      <div id="header">
        <h1>
          <a href="/" rel="home">nutrun</a>
        </h1>
      </div>
      <div id="menu">
        <ul>
          <li>
            <a href="/" rel="home">Home</a>
          </li>
          <li>
            <a href="http://twitter.com/nutrun">Twitter</a>
          </li>
          <li>
            <a href="/feed.rss">RSS</a>
          </li>
          <li>
            <a href="/feed.atom">Atom</a>
          </li>
        </ul>
      </div>
      <div id="content">
        <div class="article">
          <div>
            <span class="date">Oct 26 2008</span>
          </div>
          <h2 class="title">Parallelize by process</h2>
          <p>Performing computations in parallel is a popular technique for improving application performance and can be achieved in a number of ways, most commonly by employing threads or by splitting workload in a number of concurrent processes.</p>
          <p>Memory usage is often a headache with large dataset computations. While memory optimization is something to be sought after, tracking down memory leaks can become tedious and time consuming. We can decrease the chances of a heavy job running a system's memory dry by coming up with a strategy for fragmenting the job into a number of shorter running processes. By doing so, any memory used by a worker process will be released the moment the process completes. Additionally, we can run job fragments in parallel, allow ourselves to harness the operating system's multi-core capabilities and potentially distribute worker processes over a number of physical hosts and scale out when the need arises. Smaller processes also dictate more manageable chunks of code which are easier to maintain, optimize and test.</p>
          <p>Let's look at an example where a job involves fetching a large number of categorized products from various sources and processes them for use by our own application.</p>
          <pre>&#x000A;class Job&#x000A;  def perform&#x000A;    ADDRESSES.each do |address|&#x000A;      category = load_category(address)&#x000A;      category.products.each { |product| process(product) }&#x000A;    end&#x000A;  end&#x000A;  &#x000A;  def process(product)&#x000A;    #some intensive computation&#x000A;  end&#x000A;  &#x000A;  def load_category(address)&#x000A;    #load an addressable category dataset&#x000A;  end&#x000A;end</pre>
          <p>
            Let's assume that the
            <code>ADDRESSES</code>
            constant in the example is a list consisting of entries such as
            <code>example.com/toys</code>,
            <code>example.com/phones</code>,
            <code>example.org/guitars</code>,
            etc. The job fetches the addressable by category product datasets, iterates over the products and performs a long processing operation on each. Supposing that after every possible optimization the job takes three hours to complete, we can at best run the job eight times a day. What happens if the product categories are updated more often than eight times a day and a requirement in order for our application to be successful suggests that it needs to deal with fresh data all the time?
          </p>
          <p>
            One natural split can involve creating a worker process for each address entry. We can do so by extracting the majority of the code from the
            <code>Job</code>
            class into a
            <code>Worker</code>
            class meant to run as a standalone process.
          </p>
          <pre>&#x000A;class Worker&#x000A;  def self.process_category(address)&#x000A;    category = load_category(address)&#x000A;    category.products.each { |product| process(product) }&#x000A;  end&#x000A;  &#x000A;  def self.process(product)&#x000A;    #some intensive computation&#x000A;  end&#x000A;  &#x000A;  def self.load_category(address)&#x000A;    #load an addressable category dataset&#x000A;  end&#x000A;end&#x000A;&#x000A;Worker.process_category(ARGV[0]) if ARGV.size == 1</pre>
          <p>Each worker will operate on a significantly smaller dataset and will complete much faster than the initial long running job. Any memory used by each worker will be immediately released the moment the process finishes execution.</p>
          <p>
            After the latest change,
            <code>Job</code>
            can take on the role of instrumenting the worker processes. We start by only allowing an arbitrary maximum number of concurrent workers, three in this case.
          </p>
          <pre>&#x000A;require "thread"&#x000A;&#x000A;class Job&#x000A;  def initialize&#x000A;    @worker_count, @mutex = 3, Mutex.new&#x000A;  end&#x000A;&#x000A;  def perform&#x000A;    ADRESSESES.each do |address|&#x000A;      sleep 0.1 until @worker_count > 0&#x000A;      @worker_count -= 1&#x000A;      Thread.new do&#x000A;        system("ruby worker.rb #{address}")&#x000A;        @mutex.synchronize {@worker_count += 1}&#x000A;      end&#x000A;    end&#x000A;  end&#x000A;end</pre>
          <p>
            At this point it is a good idea to run the job and monitor the time it takes for it to complete while also measuring system resource usage. This way we can determine the optimal number of concurrent worker processes based on the system's specs. Once available resources have been exhausted and both
            <code>Job</code>
            and
            <code>Worker</code>
            have been sufficiently optimized, we can start thinking about running workers on separate physical nodes.
          </p>
        </div>
      </div>
      <div id="footer">
        All content &copy; 2006-2011 George Malamidis, nutrun
      </div>
    </div>
  </body>
</html>
